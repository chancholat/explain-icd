name: masked_pooling_aux_loss
configs:
  lambda_aux: 0.3
  stop_gradient_unselected: true
  mask_pooling: true # set false for full-context forward + stop-grad
  soft_alpha: 0.0 # e.g., 0.1 for soft pooling
  fallback_to_full_attention_if_empty: true
  use_token_loss: true # Whether to compute the token-level auxiliary loss
  reference_model_path: null # Path to a reference model checkpoint to compute explanation thresholds
  evidence_selection_strategy: "auto" # Strategy for evidence selection: "auto", "evidence_ids", "reference_model", "training_model"
  explanation_method: "laat" # Method to use for token attributions: "laat", "alti", "attention_rollout", "gradient_attention", "deeplift", "integrated_gradients", etc.
  